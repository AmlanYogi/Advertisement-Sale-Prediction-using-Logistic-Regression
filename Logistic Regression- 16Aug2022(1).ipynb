{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40bd11a5",
   "metadata": {},
   "source": [
    "# Importing Liabraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "265a2861",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c300d97",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e868f494",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"DigitalAd_dataset.csv\")\n",
    "df = dataset.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61ab580",
   "metadata": {},
   "source": [
    "# Summarize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "568763fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 3)\n",
      "   Age  Salary  Status\n",
      "0   18   82000       0\n",
      "1   29   80000       0\n",
      "2   47   25000       1\n",
      "3   45   26000       1\n",
      "4   46   28000       1\n"
     ]
    }
   ],
   "source": [
    "print(dataset.shape)\n",
    "print(dataset.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90227f96",
   "metadata": {},
   "source": [
    "# Segregate dataset into X(Input/Independent variable) & Y (Output/ Dependent Variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f11b701",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    18,  82000],\n",
       "       [    29,  80000],\n",
       "       [    47,  25000],\n",
       "       [    45,  26000],\n",
       "       [    46,  28000],\n",
       "       [    48,  29000],\n",
       "       [    45,  22000],\n",
       "       [    47,  49000],\n",
       "       [    48,  41000],\n",
       "       [    45,  22000],\n",
       "       [    46,  23000],\n",
       "       [    47,  20000],\n",
       "       [    49,  28000],\n",
       "       [    47,  30000],\n",
       "       [    29,  43000],\n",
       "       [    31,  18000],\n",
       "       [    31,  74000],\n",
       "       [    27, 137000],\n",
       "       [    21,  16000],\n",
       "       [    28,  44000],\n",
       "       [    27,  90000],\n",
       "       [    35,  27000],\n",
       "       [    33,  28000],\n",
       "       [    30,  49000],\n",
       "       [    26,  72000],\n",
       "       [    27,  31000],\n",
       "       [    27,  17000],\n",
       "       [    33,  51000],\n",
       "       [    35, 108000],\n",
       "       [    30,  15000],\n",
       "       [    28,  84000],\n",
       "       [    23,  20000],\n",
       "       [    25,  79000],\n",
       "       [    27,  54000],\n",
       "       [    30, 135000],\n",
       "       [    31,  89000],\n",
       "       [    24,  32000],\n",
       "       [    18,  44000],\n",
       "       [    29,  83000],\n",
       "       [    35,  23000],\n",
       "       [    27,  58000],\n",
       "       [    24,  55000],\n",
       "       [    23,  48000],\n",
       "       [    28,  79000],\n",
       "       [    22,  18000],\n",
       "       [    32, 117000],\n",
       "       [    27,  20000],\n",
       "       [    25,  87000],\n",
       "       [    23,  66000],\n",
       "       [    32, 120000],\n",
       "       [    59,  83000],\n",
       "       [    24,  58000],\n",
       "       [    24,  19000],\n",
       "       [    23,  82000],\n",
       "       [    22,  63000],\n",
       "       [    31,  68000],\n",
       "       [    25,  80000],\n",
       "       [    24,  27000],\n",
       "       [    20,  23000],\n",
       "       [    33, 113000],\n",
       "       [    32,  18000],\n",
       "       [    34, 112000],\n",
       "       [    18,  52000],\n",
       "       [    22,  27000],\n",
       "       [    28,  87000],\n",
       "       [    26,  17000],\n",
       "       [    30,  80000],\n",
       "       [    39,  42000],\n",
       "       [    20,  49000],\n",
       "       [    35,  88000],\n",
       "       [    30,  62000],\n",
       "       [    31, 118000],\n",
       "       [    24,  55000],\n",
       "       [    28,  85000],\n",
       "       [    26,  81000],\n",
       "       [    35,  50000],\n",
       "       [    22,  81000],\n",
       "       [    30, 116000],\n",
       "       [    26,  15000],\n",
       "       [    29,  28000],\n",
       "       [    29,  83000],\n",
       "       [    35,  44000],\n",
       "       [    35,  25000],\n",
       "       [    28, 123000],\n",
       "       [    35,  73000],\n",
       "       [    28,  37000],\n",
       "       [    27,  88000],\n",
       "       [    28,  59000],\n",
       "       [    32,  86000],\n",
       "       [    33, 149000],\n",
       "       [    19,  21000],\n",
       "       [    21,  72000],\n",
       "       [    26,  35000],\n",
       "       [    27,  89000],\n",
       "       [    26,  86000],\n",
       "       [    38,  80000],\n",
       "       [    39,  71000],\n",
       "       [    37,  71000],\n",
       "       [    38,  61000],\n",
       "       [    37,  55000],\n",
       "       [    42,  80000],\n",
       "       [    40,  57000],\n",
       "       [    35,  75000],\n",
       "       [    36,  52000],\n",
       "       [    40,  59000],\n",
       "       [    41,  59000],\n",
       "       [    36,  75000],\n",
       "       [    37,  72000],\n",
       "       [    40,  75000],\n",
       "       [    35,  53000],\n",
       "       [    41,  51000],\n",
       "       [    39,  61000],\n",
       "       [    42,  65000],\n",
       "       [    26,  32000],\n",
       "       [    30,  17000],\n",
       "       [    26,  84000],\n",
       "       [    31,  58000],\n",
       "       [    33,  31000],\n",
       "       [    30,  87000],\n",
       "       [    21,  68000],\n",
       "       [    28,  55000],\n",
       "       [    23,  63000],\n",
       "       [    20,  82000],\n",
       "       [    30, 107000],\n",
       "       [    28,  59000],\n",
       "       [    19,  25000],\n",
       "       [    19,  85000],\n",
       "       [    18,  68000],\n",
       "       [    35,  59000],\n",
       "       [    30,  89000],\n",
       "       [    34,  25000],\n",
       "       [    24,  89000],\n",
       "       [    27,  96000],\n",
       "       [    41,  30000],\n",
       "       [    29,  61000],\n",
       "       [    20,  74000],\n",
       "       [    26,  15000],\n",
       "       [    41,  45000],\n",
       "       [    31,  76000],\n",
       "       [    36,  50000],\n",
       "       [    40,  47000],\n",
       "       [    31,  15000],\n",
       "       [    46,  59000],\n",
       "       [    29,  75000],\n",
       "       [    26,  30000],\n",
       "       [    32, 135000],\n",
       "       [    32, 100000],\n",
       "       [    25,  90000],\n",
       "       [    37,  33000],\n",
       "       [    35,  38000],\n",
       "       [    33,  69000],\n",
       "       [    18,  86000],\n",
       "       [    22,  55000],\n",
       "       [    35,  71000],\n",
       "       [    29, 148000],\n",
       "       [    29,  47000],\n",
       "       [    21,  88000],\n",
       "       [    34, 115000],\n",
       "       [    26, 118000],\n",
       "       [    34,  43000],\n",
       "       [    34,  72000],\n",
       "       [    23,  28000],\n",
       "       [    35,  47000],\n",
       "       [    25,  22000],\n",
       "       [    24,  23000],\n",
       "       [    31,  34000],\n",
       "       [    26,  16000],\n",
       "       [    31,  71000],\n",
       "       [    32, 117000],\n",
       "       [    33,  43000],\n",
       "       [    33,  60000],\n",
       "       [    31,  66000],\n",
       "       [    20,  82000],\n",
       "       [    33,  41000],\n",
       "       [    35,  72000],\n",
       "       [    28,  32000],\n",
       "       [    24,  84000],\n",
       "       [    19,  26000],\n",
       "       [    29,  43000],\n",
       "       [    19,  70000],\n",
       "       [    28,  89000],\n",
       "       [    34,  43000],\n",
       "       [    30,  79000],\n",
       "       [    20,  36000],\n",
       "       [    26,  80000],\n",
       "       [    35,  22000],\n",
       "       [    35,  39000],\n",
       "       [    49,  74000],\n",
       "       [    39, 134000],\n",
       "       [    41,  71000],\n",
       "       [    58, 101000],\n",
       "       [    47,  47000],\n",
       "       [    55, 130000],\n",
       "       [    52, 114000],\n",
       "       [    40, 142000],\n",
       "       [    46,  22000],\n",
       "       [    48,  96000],\n",
       "       [    52, 150000],\n",
       "       [    59,  42000],\n",
       "       [    35,  58000],\n",
       "       [    47,  43000],\n",
       "       [    60, 108000],\n",
       "       [    49,  65000],\n",
       "       [    40,  78000],\n",
       "       [    46,  96000],\n",
       "       [    59, 143000],\n",
       "       [    41,  80000],\n",
       "       [    35,  91000],\n",
       "       [    37, 144000],\n",
       "       [    60, 102000],\n",
       "       [    35,  60000],\n",
       "       [    37,  53000],\n",
       "       [    36, 126000],\n",
       "       [    56, 133000],\n",
       "       [    40,  72000],\n",
       "       [    42,  80000],\n",
       "       [    35, 147000],\n",
       "       [    39,  42000],\n",
       "       [    40, 107000],\n",
       "       [    49,  86000],\n",
       "       [    38, 112000],\n",
       "       [    46,  79000],\n",
       "       [    40,  57000],\n",
       "       [    37,  80000],\n",
       "       [    46,  82000],\n",
       "       [    53, 143000],\n",
       "       [    42, 149000],\n",
       "       [    38,  59000],\n",
       "       [    50,  88000],\n",
       "       [    56, 104000],\n",
       "       [    41,  72000],\n",
       "       [    51, 146000],\n",
       "       [    35,  50000],\n",
       "       [    57, 122000],\n",
       "       [    41,  52000],\n",
       "       [    35,  97000],\n",
       "       [    44,  39000],\n",
       "       [    37,  52000],\n",
       "       [    48, 134000],\n",
       "       [    37, 146000],\n",
       "       [    50,  44000],\n",
       "       [    52,  90000],\n",
       "       [    41,  72000],\n",
       "       [    40,  57000],\n",
       "       [    58,  95000],\n",
       "       [    45, 131000],\n",
       "       [    35,  77000],\n",
       "       [    36, 144000],\n",
       "       [    55, 125000],\n",
       "       [    35,  72000],\n",
       "       [    48,  90000],\n",
       "       [    42, 108000],\n",
       "       [    40,  75000],\n",
       "       [    37,  74000],\n",
       "       [    47, 144000],\n",
       "       [    40,  61000],\n",
       "       [    43, 133000],\n",
       "       [    59,  76000],\n",
       "       [    60,  42000],\n",
       "       [    39, 106000],\n",
       "       [    57,  26000],\n",
       "       [    57,  74000],\n",
       "       [    38,  71000],\n",
       "       [    49,  88000],\n",
       "       [    52,  38000],\n",
       "       [    50,  36000],\n",
       "       [    59,  88000],\n",
       "       [    35,  61000],\n",
       "       [    37,  70000],\n",
       "       [    52,  21000],\n",
       "       [    48, 141000],\n",
       "       [    37,  93000],\n",
       "       [    37,  62000],\n",
       "       [    48, 138000],\n",
       "       [    41,  79000],\n",
       "       [    37,  78000],\n",
       "       [    39, 134000],\n",
       "       [    49,  89000],\n",
       "       [    55,  39000],\n",
       "       [    37,  77000],\n",
       "       [    35,  57000],\n",
       "       [    36,  63000],\n",
       "       [    42,  73000],\n",
       "       [    43, 112000],\n",
       "       [    45,  79000],\n",
       "       [    46, 117000],\n",
       "       [    58,  38000],\n",
       "       [    48,  74000],\n",
       "       [    37, 137000],\n",
       "       [    37,  79000],\n",
       "       [    40,  60000],\n",
       "       [    42,  54000],\n",
       "       [    51, 134000],\n",
       "       [    47, 113000],\n",
       "       [    36, 125000],\n",
       "       [    38,  50000],\n",
       "       [    42,  70000],\n",
       "       [    39,  96000],\n",
       "       [    38,  50000],\n",
       "       [    49, 141000],\n",
       "       [    39,  79000],\n",
       "       [    39,  75000],\n",
       "       [    54, 104000],\n",
       "       [    35,  55000],\n",
       "       [    45,  32000],\n",
       "       [    36,  60000],\n",
       "       [    52, 138000],\n",
       "       [    53,  82000],\n",
       "       [    41,  52000],\n",
       "       [    48,  30000],\n",
       "       [    48, 131000],\n",
       "       [    41,  60000],\n",
       "       [    41,  72000],\n",
       "       [    42,  75000],\n",
       "       [    36, 118000],\n",
       "       [    47, 107000],\n",
       "       [    38,  51000],\n",
       "       [    48, 119000],\n",
       "       [    42,  65000],\n",
       "       [    40,  65000],\n",
       "       [    57,  60000],\n",
       "       [    36,  54000],\n",
       "       [    58, 144000],\n",
       "       [    35,  79000],\n",
       "       [    38,  55000],\n",
       "       [    39, 122000],\n",
       "       [    53, 104000],\n",
       "       [    35,  75000],\n",
       "       [    38,  65000],\n",
       "       [    47,  51000],\n",
       "       [    47, 105000],\n",
       "       [    41,  63000],\n",
       "       [    53,  72000],\n",
       "       [    54, 108000],\n",
       "       [    39,  77000],\n",
       "       [    38,  61000],\n",
       "       [    38, 113000],\n",
       "       [    37,  75000],\n",
       "       [    42,  90000],\n",
       "       [    37,  57000],\n",
       "       [    36,  99000],\n",
       "       [    60,  34000],\n",
       "       [    54,  70000],\n",
       "       [    41,  72000],\n",
       "       [    40,  71000],\n",
       "       [    42,  54000],\n",
       "       [    43, 129000],\n",
       "       [    53,  34000],\n",
       "       [    47,  50000],\n",
       "       [    42,  79000],\n",
       "       [    42, 104000],\n",
       "       [    59,  29000],\n",
       "       [    58,  47000],\n",
       "       [    46,  88000],\n",
       "       [    38,  71000],\n",
       "       [    54,  26000],\n",
       "       [    60,  46000],\n",
       "       [    60,  83000],\n",
       "       [    39,  73000],\n",
       "       [    59, 130000],\n",
       "       [    37,  80000],\n",
       "       [    46,  32000],\n",
       "       [    46,  74000],\n",
       "       [    42,  53000],\n",
       "       [    41,  87000],\n",
       "       [    58,  23000],\n",
       "       [    42,  64000],\n",
       "       [    48,  33000],\n",
       "       [    44, 139000],\n",
       "       [    49,  28000],\n",
       "       [    57,  33000],\n",
       "       [    56,  60000],\n",
       "       [    49,  39000],\n",
       "       [    39,  71000],\n",
       "       [    47,  34000],\n",
       "       [    48,  35000],\n",
       "       [    48,  33000],\n",
       "       [    47,  23000],\n",
       "       [    45,  45000],\n",
       "       [    60,  42000],\n",
       "       [    39,  59000],\n",
       "       [    46,  41000],\n",
       "       [    51,  23000],\n",
       "       [    50,  20000],\n",
       "       [    36,  33000],\n",
       "       [    49,  36000],\n",
       "       [    19,  19000],\n",
       "       [    35,  20000],\n",
       "       [    26,  43000],\n",
       "       [    27,  57000],\n",
       "       [    19,  76000],\n",
       "       [    27,  58000],\n",
       "       [    27,  84000],\n",
       "       [    32, 150000],\n",
       "       [    25,  33000],\n",
       "       [    35,  65000],\n",
       "       [    26,  80000],\n",
       "       [    26,  52000],\n",
       "       [    20,  86000],\n",
       "       [    32,  18000]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = dataset.iloc[:, :-1].values\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ecd52e1a",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,\n",
       "       0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,\n",
       "       0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,\n",
       "       0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,\n",
       "       1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,\n",
       "       0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,\n",
       "       1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = dataset.iloc[:, -1].values\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa53865",
   "metadata": {},
   "source": [
    "# Splitting Dataset into Train & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "375ccb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,Y, test_size = 0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a020604",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    48,  90000],\n",
       "       [    22,  27000],\n",
       "       [    41,  72000],\n",
       "       [    34,  43000],\n",
       "       [    43, 112000],\n",
       "       [    36,  99000],\n",
       "       [    35,  44000],\n",
       "       [    42,  79000],\n",
       "       [    35,  71000],\n",
       "       [    38,  50000],\n",
       "       [    39,  71000],\n",
       "       [    33, 149000],\n",
       "       [    42,  70000],\n",
       "       [    35,  27000],\n",
       "       [    58,  23000],\n",
       "       [    35, 147000],\n",
       "       [    39, 106000],\n",
       "       [    32, 120000],\n",
       "       [    48, 134000],\n",
       "       [    41,  72000],\n",
       "       [    26,  35000],\n",
       "       [    37,  80000],\n",
       "       [    41,  52000],\n",
       "       [    35,  50000],\n",
       "       [    47,  23000],\n",
       "       [    46,  59000],\n",
       "       [    33,  28000],\n",
       "       [    40,  75000],\n",
       "       [    42, 104000],\n",
       "       [    32, 117000],\n",
       "       [    33,  69000],\n",
       "       [    32, 150000],\n",
       "       [    30,  80000],\n",
       "       [    50,  44000],\n",
       "       [    40, 107000],\n",
       "       [    40,  57000],\n",
       "       [    41,  60000],\n",
       "       [    40, 142000],\n",
       "       [    53, 104000],\n",
       "       [    27, 137000],\n",
       "       [    24,  23000],\n",
       "       [    35,  39000],\n",
       "       [    28,  84000],\n",
       "       [    30,  17000],\n",
       "       [    49,  88000],\n",
       "       [    36,  52000],\n",
       "       [    39,  73000],\n",
       "       [    45, 131000],\n",
       "       [    35,  97000],\n",
       "       [    31,  58000],\n",
       "       [    47, 105000],\n",
       "       [    28,  55000],\n",
       "       [    37,  79000],\n",
       "       [    42,  65000],\n",
       "       [    42,  80000],\n",
       "       [    26,  15000],\n",
       "       [    37,  78000],\n",
       "       [    19,  85000],\n",
       "       [    59,  42000],\n",
       "       [    49, 141000],\n",
       "       [    36,  63000],\n",
       "       [    41,  30000],\n",
       "       [    27,  54000],\n",
       "       [    45,  45000],\n",
       "       [    35,  47000],\n",
       "       [    30, 135000],\n",
       "       [    51, 146000],\n",
       "       [    37,  71000],\n",
       "       [    28,  37000],\n",
       "       [    34, 112000],\n",
       "       [    31,  71000],\n",
       "       [    42,  73000],\n",
       "       [    47,  43000],\n",
       "       [    27,  58000],\n",
       "       [    41,  72000],\n",
       "       [    48,  74000],\n",
       "       [    40,  75000],\n",
       "       [    27,  20000],\n",
       "       [    57,  60000],\n",
       "       [    26,  80000],\n",
       "       [    46,  82000],\n",
       "       [    28,  85000],\n",
       "       [    41,  45000],\n",
       "       [    46,  41000],\n",
       "       [    38, 112000],\n",
       "       [    35,  60000],\n",
       "       [    30,  15000],\n",
       "       [    34,  43000],\n",
       "       [    37,  80000],\n",
       "       [    37,  93000],\n",
       "       [    24,  58000],\n",
       "       [    38,  65000],\n",
       "       [    58,  47000],\n",
       "       [    33,  51000],\n",
       "       [    47,  25000],\n",
       "       [    39,  42000],\n",
       "       [    21,  88000],\n",
       "       [    36, 126000],\n",
       "       [    48,  33000],\n",
       "       [    46,  79000],\n",
       "       [    31,  76000],\n",
       "       [    44,  39000],\n",
       "       [    49,  86000],\n",
       "       [    41,  79000],\n",
       "       [    55,  39000],\n",
       "       [    53,  82000],\n",
       "       [    37, 146000],\n",
       "       [    31,  89000],\n",
       "       [    46,  96000],\n",
       "       [    27,  84000],\n",
       "       [    39,  42000],\n",
       "       [    26,  72000],\n",
       "       [    53,  72000],\n",
       "       [    22,  18000],\n",
       "       [    52,  90000],\n",
       "       [    30,  89000],\n",
       "       [    27,  89000],\n",
       "       [    39,  61000],\n",
       "       [    26,  16000],\n",
       "       [    27,  57000],\n",
       "       [    50,  20000],\n",
       "       [    54,  70000],\n",
       "       [    27,  58000],\n",
       "       [    21,  16000],\n",
       "       [    45,  79000],\n",
       "       [    29,  28000],\n",
       "       [    35,  72000],\n",
       "       [    25,  33000],\n",
       "       [    31, 118000],\n",
       "       [    47,  30000],\n",
       "       [    48,  33000],\n",
       "       [    56, 133000],\n",
       "       [    49,  36000],\n",
       "       [    26,  43000],\n",
       "       [    50,  88000],\n",
       "       [    34,  72000],\n",
       "       [    40,  59000],\n",
       "       [    23,  28000],\n",
       "       [    28, 123000],\n",
       "       [    41,  71000],\n",
       "       [    26,  52000],\n",
       "       [    30,  87000],\n",
       "       [    47, 144000],\n",
       "       [    39, 134000],\n",
       "       [    37, 144000],\n",
       "       [    48,  35000],\n",
       "       [    41,  51000],\n",
       "       [    35,  38000],\n",
       "       [    34, 115000],\n",
       "       [    22,  55000],\n",
       "       [    31,  74000],\n",
       "       [    52,  21000],\n",
       "       [    35,  50000],\n",
       "       [    35,  53000],\n",
       "       [    35,  75000],\n",
       "       [    59, 143000],\n",
       "       [    47, 107000],\n",
       "       [    36,  50000],\n",
       "       [    37,  52000],\n",
       "       [    40,  65000],\n",
       "       [    55, 125000],\n",
       "       [    41,  52000],\n",
       "       [    28,  44000],\n",
       "       [    42, 149000],\n",
       "       [    52, 138000],\n",
       "       [    45,  26000],\n",
       "       [    39, 134000],\n",
       "       [    19,  25000],\n",
       "       [    30, 116000],\n",
       "       [    26,  80000],\n",
       "       [    39,  75000],\n",
       "       [    60,  42000],\n",
       "       [    43, 129000],\n",
       "       [    30,  79000],\n",
       "       [    60,  46000],\n",
       "       [    29,  83000],\n",
       "       [    60,  42000],\n",
       "       [    47,  20000],\n",
       "       [    38,  50000],\n",
       "       [    27,  88000],\n",
       "       [    59,  88000],\n",
       "       [    24,  32000],\n",
       "       [    51,  23000],\n",
       "       [    20,  23000],\n",
       "       [    24,  55000],\n",
       "       [    48, 141000],\n",
       "       [    59,  83000],\n",
       "       [    60, 102000],\n",
       "       [    48, 119000],\n",
       "       [    38,  51000],\n",
       "       [    41,  63000],\n",
       "       [    30, 107000],\n",
       "       [    40,  57000],\n",
       "       [    18,  52000],\n",
       "       [    54, 104000],\n",
       "       [    34,  25000],\n",
       "       [    49,  74000],\n",
       "       [    30,  49000],\n",
       "       [    28,  79000],\n",
       "       [    18,  82000],\n",
       "       [    60, 108000],\n",
       "       [    37,  57000],\n",
       "       [    38,  61000],\n",
       "       [    35,  20000],\n",
       "       [    29,  43000],\n",
       "       [    43, 133000],\n",
       "       [    26,  86000],\n",
       "       [    49,  28000],\n",
       "       [    38,  80000],\n",
       "       [    59,  29000],\n",
       "       [    33,  43000],\n",
       "       [    35,  88000],\n",
       "       [    36,  60000],\n",
       "       [    23,  66000],\n",
       "       [    35,  91000],\n",
       "       [    37,  77000],\n",
       "       [    38,  59000],\n",
       "       [    37,  33000],\n",
       "       [    29,  75000],\n",
       "       [    28,  89000],\n",
       "       [    24,  89000],\n",
       "       [    60,  83000],\n",
       "       [    20,  86000],\n",
       "       [    38,  71000],\n",
       "       [    38,  55000],\n",
       "       [    40,  78000],\n",
       "       [    35,  73000],\n",
       "       [    23,  63000],\n",
       "       [    42,  54000],\n",
       "       [    42,  64000],\n",
       "       [    21,  72000],\n",
       "       [    35,  25000],\n",
       "       [    35,  61000],\n",
       "       [    21,  68000],\n",
       "       [    42,  54000],\n",
       "       [    24,  27000],\n",
       "       [    36,  54000],\n",
       "       [    59,  76000],\n",
       "       [    54,  26000],\n",
       "       [    23,  48000],\n",
       "       [    41,  59000],\n",
       "       [    44, 139000],\n",
       "       [    48, 138000],\n",
       "       [    46,  88000],\n",
       "       [    29,  83000],\n",
       "       [    23,  82000],\n",
       "       [    53,  34000],\n",
       "       [    35,  59000],\n",
       "       [    40,  60000],\n",
       "       [    35, 108000],\n",
       "       [    20,  36000],\n",
       "       [    25,  22000],\n",
       "       [    18,  86000],\n",
       "       [    58,  95000],\n",
       "       [    49,  65000],\n",
       "       [    23,  20000],\n",
       "       [    25,  79000],\n",
       "       [    18,  68000],\n",
       "       [    35,  22000],\n",
       "       [    49,  39000],\n",
       "       [    37, 137000],\n",
       "       [    46,  74000],\n",
       "       [    25,  90000],\n",
       "       [    46, 117000],\n",
       "       [    57,  33000],\n",
       "       [    19,  26000],\n",
       "       [    37,  55000],\n",
       "       [    42,  90000],\n",
       "       [    38,  61000],\n",
       "       [    52, 150000],\n",
       "       [    40,  57000],\n",
       "       [    26,  84000],\n",
       "       [    50,  36000],\n",
       "       [    24,  55000],\n",
       "       [    54, 108000],\n",
       "       [    27,  31000],\n",
       "       [    31,  34000],\n",
       "       [    37,  75000],\n",
       "       [    36,  33000],\n",
       "       [    35,  72000],\n",
       "       [    19,  19000],\n",
       "       [    35,  23000],\n",
       "       [    52, 114000],\n",
       "       [    36, 118000],\n",
       "       [    32,  86000],\n",
       "       [    30,  62000],\n",
       "       [    28,  59000],\n",
       "       [    51, 134000],\n",
       "       [    41,  72000],\n",
       "       [    49,  89000],\n",
       "       [    37,  53000],\n",
       "       [    45,  22000],\n",
       "       [    59, 130000],\n",
       "       [    46,  22000],\n",
       "       [    42, 108000],\n",
       "       [    35,  79000],\n",
       "       [    55, 130000],\n",
       "       [    33,  31000],\n",
       "       [    25,  87000],\n",
       "       [    20,  82000]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "206c8abd",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    27,  96000],\n",
       "       [    48,  30000],\n",
       "       [    60,  34000],\n",
       "       [    48,  96000],\n",
       "       [    35,  77000],\n",
       "       [    32,  18000],\n",
       "       [    29,  47000],\n",
       "       [    57,  74000],\n",
       "       [    31,  15000],\n",
       "       [    40,  72000],\n",
       "       [    18,  44000],\n",
       "       [    29,  61000],\n",
       "       [    26,  32000],\n",
       "       [    47,  50000],\n",
       "       [    49,  28000],\n",
       "       [    33, 113000],\n",
       "       [    47, 113000],\n",
       "       [    40,  47000],\n",
       "       [    41,  80000],\n",
       "       [    35,  58000],\n",
       "       [    24,  84000],\n",
       "       [    37,  70000],\n",
       "       [    28,  59000],\n",
       "       [    40,  71000],\n",
       "       [    28,  32000],\n",
       "       [    42,  75000],\n",
       "       [    26,  15000],\n",
       "       [    31,  18000],\n",
       "       [    58,  38000],\n",
       "       [    35,  75000],\n",
       "       [    33,  60000],\n",
       "       [    35,  55000],\n",
       "       [    39,  77000],\n",
       "       [    53, 143000],\n",
       "       [    26,  17000],\n",
       "       [    22,  81000],\n",
       "       [    19,  21000],\n",
       "       [    33,  41000],\n",
       "       [    19,  70000],\n",
       "       [    32,  18000],\n",
       "       [    42,  80000],\n",
       "       [    58, 144000],\n",
       "       [    45,  22000],\n",
       "       [    29,  80000],\n",
       "       [    39,  96000],\n",
       "       [    22,  63000],\n",
       "       [    47,  34000],\n",
       "       [    40,  61000],\n",
       "       [    26, 118000],\n",
       "       [    57, 122000],\n",
       "       [    36, 144000],\n",
       "       [    26,  30000],\n",
       "       [    36, 125000],\n",
       "       [    31,  66000],\n",
       "       [    35,  57000],\n",
       "       [    42,  65000],\n",
       "       [    24,  19000],\n",
       "       [    46,  23000],\n",
       "       [    27,  17000],\n",
       "       [    32, 117000],\n",
       "       [    26,  81000],\n",
       "       [    37,  62000],\n",
       "       [    46,  28000],\n",
       "       [    38,  71000],\n",
       "       [    47,  47000],\n",
       "       [    56,  60000],\n",
       "       [    32, 135000],\n",
       "       [    48,  29000],\n",
       "       [    29, 148000],\n",
       "       [    48, 131000],\n",
       "       [    47,  49000],\n",
       "       [    57,  26000],\n",
       "       [    20,  49000],\n",
       "       [    27,  90000],\n",
       "       [    37,  72000],\n",
       "       [    29,  43000],\n",
       "       [    42,  53000],\n",
       "       [    45,  32000],\n",
       "       [    46,  32000],\n",
       "       [    47,  51000],\n",
       "       [    38, 113000],\n",
       "       [    28,  87000],\n",
       "       [    31,  68000],\n",
       "       [    36,  75000],\n",
       "       [    39,  79000],\n",
       "       [    56, 104000],\n",
       "       [    20,  82000],\n",
       "       [    39,  71000],\n",
       "       [    35,  65000],\n",
       "       [    39, 122000],\n",
       "       [    39,  59000],\n",
       "       [    37,  74000],\n",
       "       [    25,  80000],\n",
       "       [    48,  41000],\n",
       "       [    58, 101000],\n",
       "       [    32, 100000],\n",
       "       [    20,  74000],\n",
       "       [    19,  76000],\n",
       "       [    52,  38000],\n",
       "       [    41,  87000]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e59d219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If random_state not given, then every time it'll take randomly 75%-80% new data for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679097b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_size is mandatory & random_state is optional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fa444b",
   "metadata": {},
   "source": [
    "# Feature Scaling"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e5cd88de",
   "metadata": {},
   "source": [
    "# Feature scaling is used when there are different units of data availble for comparison.\n",
    "\n",
    "# We scale our data to make all the features contribute equally to the result.\n",
    "\n",
    "# fit_transform = used for training of data. Fit method is calculating the mean and variance of each of the features(each column separately will be taken) present in our data.\n",
    "\n",
    "# transform = used for testing of data. transform method is transforming all the features (all columns combined) using the respective mean and variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd00a8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "# Standardization method\n",
    "\n",
    "# StandardScaler method is best if not requiring output data in 0-1\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "sc = StandardScaler()\n",
    "\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1166fd76",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.97446763,  0.56400811],\n",
       "       [-1.52088611, -1.28151705],\n",
       "       [ 0.30264162,  0.03671521],\n",
       "       [-0.36918439, -0.81281224],\n",
       "       [ 0.49459191,  1.20847722],\n",
       "       [-0.1772341 ,  0.82765456],\n",
       "       [-0.27320924, -0.78351819],\n",
       "       [ 0.39861676,  0.24177356],\n",
       "       [-0.27320924,  0.00742116],\n",
       "       [ 0.01471619, -0.60775389],\n",
       "       [ 0.11069133,  0.00742116],\n",
       "       [-0.46515953,  2.29235707],\n",
       "       [ 0.39861676, -0.02187289],\n",
       "       [-0.27320924, -1.28151705],\n",
       "       [ 1.93421906, -1.39869325],\n",
       "       [-0.27320924,  2.23376897],\n",
       "       [ 0.11069133,  1.03271291],\n",
       "       [-0.56113467,  1.44282962],\n",
       "       [ 0.97446763,  1.85294632],\n",
       "       [ 0.30264162,  0.03671521],\n",
       "       [-1.13698554, -1.04716465],\n",
       "       [-0.08125895,  0.27106761],\n",
       "       [ 0.30264162, -0.54916579],\n",
       "       [-0.27320924, -0.60775389],\n",
       "       [ 0.87849248, -1.39869325],\n",
       "       [ 0.78251734, -0.34410744],\n",
       "       [-0.46515953, -1.252223  ],\n",
       "       [ 0.20666648,  0.12459736],\n",
       "       [ 0.39861676,  0.97412481],\n",
       "       [-0.56113467,  1.35494747],\n",
       "       [-0.46515953, -0.05116694],\n",
       "       [-0.56113467,  2.32165112],\n",
       "       [-0.75308496,  0.27106761],\n",
       "       [ 1.16641791, -0.78351819],\n",
       "       [ 0.20666648,  1.06200696],\n",
       "       [ 0.20666648, -0.40269554],\n",
       "       [ 0.30264162, -0.31481339],\n",
       "       [ 0.20666648,  2.08729872],\n",
       "       [ 1.45434334,  0.97412481],\n",
       "       [-1.04101039,  1.94082847],\n",
       "       [-1.32893582, -1.39869325],\n",
       "       [-0.27320924, -0.92998845],\n",
       "       [-0.94503525,  0.38824381],\n",
       "       [-0.75308496, -1.57445755],\n",
       "       [ 1.07044277,  0.50542001],\n",
       "       [-0.1772341 , -0.54916579],\n",
       "       [ 0.11069133,  0.06600926],\n",
       "       [ 0.68654219,  1.76506417],\n",
       "       [-0.27320924,  0.76906646],\n",
       "       [-0.65710982, -0.37340149],\n",
       "       [ 0.87849248,  1.00341886],\n",
       "       [-0.94503525, -0.46128364],\n",
       "       [-0.08125895,  0.24177356],\n",
       "       [ 0.39861676, -0.16834314],\n",
       "       [ 0.39861676,  0.27106761],\n",
       "       [-1.13698554, -1.63304565],\n",
       "       [-0.08125895,  0.21247951],\n",
       "       [-1.80881154,  0.41753786],\n",
       "       [ 2.03019421, -0.84210629],\n",
       "       [ 1.07044277,  2.05800467],\n",
       "       [-0.1772341 , -0.22693124],\n",
       "       [ 0.30264162, -1.1936349 ],\n",
       "       [-1.04101039, -0.49057769],\n",
       "       [ 0.68654219, -0.75422414],\n",
       "       [-0.27320924, -0.69563604],\n",
       "       [-0.75308496,  1.88224037],\n",
       "       [ 1.26239306,  2.20447492],\n",
       "       [-0.08125895,  0.00742116],\n",
       "       [-0.94503525, -0.98857655],\n",
       "       [-0.36918439,  1.20847722],\n",
       "       [-0.65710982,  0.00742116],\n",
       "       [ 0.39861676,  0.06600926],\n",
       "       [ 0.87849248, -0.81281224],\n",
       "       [-1.04101039, -0.37340149],\n",
       "       [ 0.30264162,  0.03671521],\n",
       "       [ 0.97446763,  0.09530331],\n",
       "       [ 0.20666648,  0.12459736],\n",
       "       [-1.04101039, -1.4865754 ],\n",
       "       [ 1.83824392, -0.31481339],\n",
       "       [-1.13698554,  0.27106761],\n",
       "       [ 0.78251734,  0.32965571],\n",
       "       [-0.94503525,  0.41753786],\n",
       "       [ 0.30264162, -0.75422414],\n",
       "       [ 0.78251734, -0.87140035],\n",
       "       [ 0.01471619,  1.20847722],\n",
       "       [-0.27320924, -0.31481339],\n",
       "       [-0.75308496, -1.63304565],\n",
       "       [-0.36918439, -0.81281224],\n",
       "       [-0.08125895,  0.27106761],\n",
       "       [-0.08125895,  0.65189026],\n",
       "       [-1.32893582, -0.37340149],\n",
       "       [ 0.01471619, -0.16834314],\n",
       "       [ 1.93421906, -0.69563604],\n",
       "       [-0.46515953, -0.57845984],\n",
       "       [ 0.87849248, -1.34010515],\n",
       "       [ 0.11069133, -0.84210629],\n",
       "       [-1.61686125,  0.50542001],\n",
       "       [-0.1772341 ,  1.61859392],\n",
       "       [ 0.97446763, -1.10575275],\n",
       "       [ 0.78251734,  0.24177356],\n",
       "       [-0.65710982,  0.15389141],\n",
       "       [ 0.59056705, -0.92998845],\n",
       "       [ 1.07044277,  0.44683191],\n",
       "       [ 0.30264162,  0.24177356],\n",
       "       [ 1.64629363, -0.92998845],\n",
       "       [ 1.45434334,  0.32965571],\n",
       "       [-0.08125895,  2.20447492],\n",
       "       [-0.65710982,  0.53471406],\n",
       "       [ 0.78251734,  0.73977241],\n",
       "       [-1.04101039,  0.38824381],\n",
       "       [ 0.11069133, -0.84210629],\n",
       "       [-1.13698554,  0.03671521],\n",
       "       [ 1.45434334,  0.03671521],\n",
       "       [-1.52088611, -1.5451635 ],\n",
       "       [ 1.3583682 ,  0.56400811],\n",
       "       [-0.75308496,  0.53471406],\n",
       "       [-1.04101039,  0.53471406],\n",
       "       [ 0.11069133, -0.28551934],\n",
       "       [-1.13698554, -1.6037516 ],\n",
       "       [-1.04101039, -0.40269554],\n",
       "       [ 1.16641791, -1.4865754 ],\n",
       "       [ 1.55031849, -0.02187289],\n",
       "       [-1.04101039, -0.37340149],\n",
       "       [-1.61686125, -1.6037516 ],\n",
       "       [ 0.68654219,  0.24177356],\n",
       "       [-0.8490601 , -1.252223  ],\n",
       "       [-0.27320924,  0.03671521],\n",
       "       [-1.23296068, -1.10575275],\n",
       "       [-0.65710982,  1.38424152],\n",
       "       [ 0.87849248, -1.1936349 ],\n",
       "       [ 0.97446763, -1.10575275],\n",
       "       [ 1.74226877,  1.82365227],\n",
       "       [ 1.07044277, -1.0178706 ],\n",
       "       [-1.13698554, -0.81281224],\n",
       "       [ 1.16641791,  0.50542001],\n",
       "       [-0.36918439,  0.03671521],\n",
       "       [ 0.20666648, -0.34410744],\n",
       "       [-1.42491097, -1.252223  ],\n",
       "       [-0.94503525,  1.53071177],\n",
       "       [ 0.30264162,  0.00742116],\n",
       "       [-1.13698554, -0.54916579],\n",
       "       [-0.75308496,  0.47612596],\n",
       "       [ 0.87849248,  2.14588682],\n",
       "       [ 0.11069133,  1.85294632],\n",
       "       [-0.08125895,  2.14588682],\n",
       "       [ 0.97446763, -1.04716465],\n",
       "       [ 0.30264162, -0.57845984],\n",
       "       [-0.27320924, -0.9592825 ],\n",
       "       [-0.36918439,  1.29635937],\n",
       "       [-1.52088611, -0.46128364],\n",
       "       [-0.65710982,  0.09530331],\n",
       "       [ 1.3583682 , -1.45728135],\n",
       "       [-0.27320924, -0.60775389],\n",
       "       [-0.27320924, -0.51987174],\n",
       "       [-0.27320924,  0.12459736],\n",
       "       [ 2.03019421,  2.11659277],\n",
       "       [ 0.87849248,  1.06200696],\n",
       "       [-0.1772341 , -0.60775389],\n",
       "       [-0.08125895, -0.54916579],\n",
       "       [ 0.20666648, -0.16834314],\n",
       "       [ 1.64629363,  1.58929987],\n",
       "       [ 0.30264162, -0.54916579],\n",
       "       [-0.94503525, -0.78351819],\n",
       "       [ 0.39861676,  2.29235707],\n",
       "       [ 1.3583682 ,  1.97012252],\n",
       "       [ 0.68654219, -1.3108111 ],\n",
       "       [ 0.11069133,  1.85294632],\n",
       "       [-1.80881154, -1.34010515],\n",
       "       [-0.75308496,  1.32565342],\n",
       "       [-1.13698554,  0.27106761],\n",
       "       [ 0.11069133,  0.12459736],\n",
       "       [ 2.12616935, -0.84210629],\n",
       "       [ 0.49459191,  1.70647607],\n",
       "       [-0.75308496,  0.24177356],\n",
       "       [ 2.12616935, -0.72493009],\n",
       "       [-0.8490601 ,  0.35894976],\n",
       "       [ 2.12616935, -0.84210629],\n",
       "       [ 0.87849248, -1.4865754 ],\n",
       "       [ 0.01471619, -0.60775389],\n",
       "       [-1.04101039,  0.50542001],\n",
       "       [ 2.03019421,  0.50542001],\n",
       "       [-1.32893582, -1.1350468 ],\n",
       "       [ 1.26239306, -1.39869325],\n",
       "       [-1.7128364 , -1.39869325],\n",
       "       [-1.32893582, -0.46128364],\n",
       "       [ 0.97446763,  2.05800467],\n",
       "       [ 2.03019421,  0.35894976],\n",
       "       [ 2.12616935,  0.91553671],\n",
       "       [ 0.97446763,  1.41353557],\n",
       "       [ 0.01471619, -0.57845984],\n",
       "       [ 0.30264162, -0.22693124],\n",
       "       [-0.75308496,  1.06200696],\n",
       "       [ 0.20666648, -0.40269554],\n",
       "       [-1.90478668, -0.54916579],\n",
       "       [ 1.55031849,  0.97412481],\n",
       "       [-0.36918439, -1.34010515],\n",
       "       [ 1.07044277,  0.09530331],\n",
       "       [-0.75308496, -0.63704794],\n",
       "       [-0.94503525,  0.24177356],\n",
       "       [-1.90478668,  0.32965571],\n",
       "       [ 2.12616935,  1.09130101],\n",
       "       [-0.08125895, -0.40269554],\n",
       "       [ 0.01471619, -0.28551934],\n",
       "       [-0.27320924, -1.4865754 ],\n",
       "       [-0.8490601 , -0.81281224],\n",
       "       [ 0.49459191,  1.82365227],\n",
       "       [-1.13698554,  0.44683191],\n",
       "       [ 1.07044277, -1.252223  ],\n",
       "       [ 0.01471619,  0.27106761],\n",
       "       [ 2.03019421, -1.22292895],\n",
       "       [-0.46515953, -0.81281224],\n",
       "       [-0.27320924,  0.50542001],\n",
       "       [-0.1772341 , -0.31481339],\n",
       "       [-1.42491097, -0.13904909],\n",
       "       [-0.27320924,  0.59330216],\n",
       "       [-0.08125895,  0.18318546],\n",
       "       [ 0.01471619, -0.34410744],\n",
       "       [-0.08125895, -1.10575275],\n",
       "       [-0.8490601 ,  0.12459736],\n",
       "       [-0.94503525,  0.53471406],\n",
       "       [-1.32893582,  0.53471406],\n",
       "       [ 2.12616935,  0.35894976],\n",
       "       [-1.7128364 ,  0.44683191],\n",
       "       [ 0.01471619,  0.00742116],\n",
       "       [ 0.01471619, -0.46128364],\n",
       "       [ 0.20666648,  0.21247951],\n",
       "       [-0.27320924,  0.06600926],\n",
       "       [-1.42491097, -0.22693124],\n",
       "       [ 0.39861676, -0.49057769],\n",
       "       [ 0.39861676, -0.19763719],\n",
       "       [-1.61686125,  0.03671521],\n",
       "       [-0.27320924, -1.34010515],\n",
       "       [-0.27320924, -0.28551934],\n",
       "       [-1.61686125, -0.08046099],\n",
       "       [ 0.39861676, -0.49057769],\n",
       "       [-1.32893582, -1.28151705],\n",
       "       [-0.1772341 , -0.49057769],\n",
       "       [ 2.03019421,  0.15389141],\n",
       "       [ 1.55031849, -1.3108111 ],\n",
       "       [-1.42491097, -0.66634199],\n",
       "       [ 0.30264162, -0.34410744],\n",
       "       [ 0.59056705,  1.99941657],\n",
       "       [ 0.97446763,  1.97012252],\n",
       "       [ 0.78251734,  0.50542001],\n",
       "       [-0.8490601 ,  0.35894976],\n",
       "       [-1.42491097,  0.32965571],\n",
       "       [ 1.45434334, -1.0764587 ],\n",
       "       [-0.27320924, -0.34410744],\n",
       "       [ 0.20666648, -0.31481339],\n",
       "       [-0.27320924,  1.09130101],\n",
       "       [-1.7128364 , -1.0178706 ],\n",
       "       [-1.23296068, -1.4279873 ],\n",
       "       [-1.90478668,  0.44683191],\n",
       "       [ 1.93421906,  0.71047836],\n",
       "       [ 1.07044277, -0.16834314],\n",
       "       [-1.42491097, -1.4865754 ],\n",
       "       [-1.23296068,  0.24177356],\n",
       "       [-1.90478668, -0.08046099],\n",
       "       [-0.27320924, -1.4279873 ],\n",
       "       [ 1.07044277, -0.92998845],\n",
       "       [-0.08125895,  1.94082847],\n",
       "       [ 0.78251734,  0.09530331],\n",
       "       [-1.23296068,  0.56400811],\n",
       "       [ 0.78251734,  1.35494747],\n",
       "       [ 1.83824392, -1.10575275],\n",
       "       [-1.80881154, -1.3108111 ],\n",
       "       [-0.08125895, -0.46128364],\n",
       "       [ 0.39861676,  0.56400811],\n",
       "       [ 0.01471619, -0.28551934],\n",
       "       [ 1.3583682 ,  2.32165112],\n",
       "       [ 0.20666648, -0.40269554],\n",
       "       [-1.13698554,  0.38824381],\n",
       "       [ 1.16641791, -1.0178706 ],\n",
       "       [-1.32893582, -0.46128364],\n",
       "       [ 1.55031849,  1.09130101],\n",
       "       [-1.04101039, -1.16434085],\n",
       "       [-0.65710982, -1.0764587 ],\n",
       "       [-0.08125895,  0.12459736],\n",
       "       [-0.1772341 , -1.10575275],\n",
       "       [-0.27320924,  0.03671521],\n",
       "       [-1.80881154, -1.51586945],\n",
       "       [-0.27320924, -1.39869325],\n",
       "       [ 1.3583682 ,  1.26706532],\n",
       "       [-0.1772341 ,  1.38424152],\n",
       "       [-0.56113467,  0.44683191],\n",
       "       [-0.75308496, -0.25622529],\n",
       "       [-0.94503525, -0.34410744],\n",
       "       [ 1.26239306,  1.85294632],\n",
       "       [ 0.30264162,  0.03671521],\n",
       "       [ 1.07044277,  0.53471406],\n",
       "       [-0.08125895, -0.51987174],\n",
       "       [ 0.68654219, -1.4279873 ],\n",
       "       [ 2.03019421,  1.73577012],\n",
       "       [ 0.78251734, -1.4279873 ],\n",
       "       [ 0.39861676,  1.09130101],\n",
       "       [-0.27320924,  0.24177356],\n",
       "       [ 1.64629363,  1.73577012],\n",
       "       [-0.46515953, -1.16434085],\n",
       "       [-1.23296068,  0.47612596],\n",
       "       [-1.7128364 ,  0.32965571]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "656ee083",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.04101039,  0.73977241],\n",
       "       [ 0.97446763, -1.1936349 ],\n",
       "       [ 2.12616935, -1.0764587 ],\n",
       "       [ 0.97446763,  0.73977241],\n",
       "       [-0.27320924,  0.18318546],\n",
       "       [-0.56113467, -1.5451635 ],\n",
       "       [-0.8490601 , -0.69563604],\n",
       "       [ 1.83824392,  0.09530331],\n",
       "       [-0.65710982, -1.63304565],\n",
       "       [ 0.20666648,  0.03671521],\n",
       "       [-1.90478668, -0.78351819],\n",
       "       [-0.8490601 , -0.28551934],\n",
       "       [-1.13698554, -1.1350468 ],\n",
       "       [ 0.87849248, -0.60775389],\n",
       "       [ 1.07044277, -1.252223  ],\n",
       "       [-0.46515953,  1.23777127],\n",
       "       [ 0.87849248,  1.23777127],\n",
       "       [ 0.20666648, -0.69563604],\n",
       "       [ 0.30264162,  0.27106761],\n",
       "       [-0.27320924, -0.37340149],\n",
       "       [-1.32893582,  0.38824381],\n",
       "       [-0.08125895, -0.02187289],\n",
       "       [-0.94503525, -0.34410744],\n",
       "       [ 0.20666648,  0.00742116],\n",
       "       [-0.94503525, -1.1350468 ],\n",
       "       [ 0.39861676,  0.12459736],\n",
       "       [-1.13698554, -1.63304565],\n",
       "       [-0.65710982, -1.5451635 ],\n",
       "       [ 1.93421906, -0.9592825 ],\n",
       "       [-0.27320924,  0.12459736],\n",
       "       [-0.46515953, -0.31481339],\n",
       "       [-0.27320924, -0.46128364],\n",
       "       [ 0.11069133,  0.18318546],\n",
       "       [ 1.45434334,  2.11659277],\n",
       "       [-1.13698554, -1.57445755],\n",
       "       [-1.52088611,  0.30036166],\n",
       "       [-1.80881154, -1.45728135],\n",
       "       [-0.46515953, -0.87140035],\n",
       "       [-1.80881154, -0.02187289],\n",
       "       [-0.56113467, -1.5451635 ],\n",
       "       [ 0.39861676,  0.27106761],\n",
       "       [ 1.93421906,  2.14588682],\n",
       "       [ 0.68654219, -1.4279873 ],\n",
       "       [-0.8490601 ,  0.27106761],\n",
       "       [ 0.11069133,  0.73977241],\n",
       "       [-1.52088611, -0.22693124],\n",
       "       [ 0.87849248, -1.0764587 ],\n",
       "       [ 0.20666648, -0.28551934],\n",
       "       [-1.13698554,  1.38424152],\n",
       "       [ 1.83824392,  1.50141772],\n",
       "       [-0.1772341 ,  2.14588682],\n",
       "       [-1.13698554, -1.1936349 ],\n",
       "       [-0.1772341 ,  1.58929987],\n",
       "       [-0.65710982, -0.13904909],\n",
       "       [-0.27320924, -0.40269554],\n",
       "       [ 0.39861676, -0.16834314],\n",
       "       [-1.32893582, -1.51586945],\n",
       "       [ 0.78251734, -1.39869325],\n",
       "       [-1.04101039, -1.57445755],\n",
       "       [-0.56113467,  1.35494747],\n",
       "       [-1.13698554,  0.30036166],\n",
       "       [-0.08125895, -0.25622529],\n",
       "       [ 0.78251734, -1.252223  ],\n",
       "       [ 0.01471619,  0.00742116],\n",
       "       [ 0.87849248, -0.69563604],\n",
       "       [ 1.74226877, -0.31481339],\n",
       "       [-0.56113467,  1.88224037],\n",
       "       [ 0.97446763, -1.22292895],\n",
       "       [-0.8490601 ,  2.26306302],\n",
       "       [ 0.97446763,  1.76506417],\n",
       "       [ 0.87849248, -0.63704794],\n",
       "       [ 1.83824392, -1.3108111 ],\n",
       "       [-1.7128364 , -0.63704794],\n",
       "       [-1.04101039,  0.56400811],\n",
       "       [-0.08125895,  0.03671521],\n",
       "       [-0.8490601 , -0.81281224],\n",
       "       [ 0.39861676, -0.51987174],\n",
       "       [ 0.68654219, -1.1350468 ],\n",
       "       [ 0.78251734, -1.1350468 ],\n",
       "       [ 0.87849248, -0.57845984],\n",
       "       [ 0.01471619,  1.23777127],\n",
       "       [-0.94503525,  0.47612596],\n",
       "       [-0.65710982, -0.08046099],\n",
       "       [-0.1772341 ,  0.12459736],\n",
       "       [ 0.11069133,  0.24177356],\n",
       "       [ 1.74226877,  0.97412481],\n",
       "       [-1.7128364 ,  0.32965571],\n",
       "       [ 0.11069133,  0.00742116],\n",
       "       [-0.27320924, -0.16834314],\n",
       "       [ 0.11069133,  1.50141772],\n",
       "       [ 0.11069133, -0.34410744],\n",
       "       [-0.08125895,  0.09530331],\n",
       "       [-1.23296068,  0.27106761],\n",
       "       [ 0.97446763, -0.87140035],\n",
       "       [ 1.93421906,  0.88624266],\n",
       "       [-0.56113467,  0.85694861],\n",
       "       [-1.7128364 ,  0.09530331],\n",
       "       [-1.80881154,  0.15389141],\n",
       "       [ 1.3583682 , -0.9592825 ],\n",
       "       [ 0.30264162,  0.47612596]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff7af2f",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "797ce269",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "# model = LogisticRegression(random_state=0) # optional\n",
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a98bafa1",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class LogisticRegression in module sklearn.linear_model._logistic:\n",
      "\n",
      "class LogisticRegression(sklearn.linear_model._base.LinearClassifierMixin, sklearn.linear_model._base.SparseCoefMixin, sklearn.base.BaseEstimator)\n",
      " |  LogisticRegression(penalty='l2', *, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='lbfgs', max_iter=100, multi_class='auto', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)\n",
      " |  \n",
      " |  Logistic Regression (aka logit, MaxEnt) classifier.\n",
      " |  \n",
      " |  In the multiclass case, the training algorithm uses the one-vs-rest (OvR)\n",
      " |  scheme if the 'multi_class' option is set to 'ovr', and uses the\n",
      " |  cross-entropy loss if the 'multi_class' option is set to 'multinomial'.\n",
      " |  (Currently the 'multinomial' option is supported only by the 'lbfgs',\n",
      " |  'sag', 'saga' and 'newton-cg' solvers.)\n",
      " |  \n",
      " |  This class implements regularized logistic regression using the\n",
      " |  'liblinear' library, 'newton-cg', 'sag', 'saga' and 'lbfgs' solvers. **Note\n",
      " |  that regularization is applied by default**. It can handle both dense\n",
      " |  and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit\n",
      " |  floats for optimal performance; any other input format will be converted\n",
      " |  (and copied).\n",
      " |  \n",
      " |  The 'newton-cg', 'sag', and 'lbfgs' solvers support only L2 regularization\n",
      " |  with primal formulation, or no regularization. The 'liblinear' solver\n",
      " |  supports both L1 and L2 regularization, with a dual formulation only for\n",
      " |  the L2 penalty. The Elastic-Net regularization is only supported by the\n",
      " |  'saga' solver.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <logistic_regression>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  penalty : {'l1', 'l2', 'elasticnet', 'none'}, default='l2'\n",
      " |      Specify the norm of the penalty:\n",
      " |  \n",
      " |      - `'none'`: no penalty is added;\n",
      " |      - `'l2'`: add a L2 penalty term and it is the default choice;\n",
      " |      - `'l1'`: add a L1 penalty term;\n",
      " |      - `'elasticnet'`: both L1 and L2 penalty terms are added.\n",
      " |  \n",
      " |      .. warning::\n",
      " |         Some penalties may not work with some solvers. See the parameter\n",
      " |         `solver` below, to know the compatibility between the penalty and\n",
      " |         solver.\n",
      " |  \n",
      " |      .. versionadded:: 0.19\n",
      " |         l1 penalty with SAGA solver (allowing 'multinomial' + L1)\n",
      " |  \n",
      " |  dual : bool, default=False\n",
      " |      Dual or primal formulation. Dual formulation is only implemented for\n",
      " |      l2 penalty with liblinear solver. Prefer dual=False when\n",
      " |      n_samples > n_features.\n",
      " |  \n",
      " |  tol : float, default=1e-4\n",
      " |      Tolerance for stopping criteria.\n",
      " |  \n",
      " |  C : float, default=1.0\n",
      " |      Inverse of regularization strength; must be a positive float.\n",
      " |      Like in support vector machines, smaller values specify stronger\n",
      " |      regularization.\n",
      " |  \n",
      " |  fit_intercept : bool, default=True\n",
      " |      Specifies if a constant (a.k.a. bias or intercept) should be\n",
      " |      added to the decision function.\n",
      " |  \n",
      " |  intercept_scaling : float, default=1\n",
      " |      Useful only when the solver 'liblinear' is used\n",
      " |      and self.fit_intercept is set to True. In this case, x becomes\n",
      " |      [x, self.intercept_scaling],\n",
      " |      i.e. a \"synthetic\" feature with constant value equal to\n",
      " |      intercept_scaling is appended to the instance vector.\n",
      " |      The intercept becomes ``intercept_scaling * synthetic_feature_weight``.\n",
      " |  \n",
      " |      Note! the synthetic feature weight is subject to l1/l2 regularization\n",
      " |      as all other features.\n",
      " |      To lessen the effect of regularization on synthetic feature weight\n",
      " |      (and therefore on the intercept) intercept_scaling has to be increased.\n",
      " |  \n",
      " |  class_weight : dict or 'balanced', default=None\n",
      " |      Weights associated with classes in the form ``{class_label: weight}``.\n",
      " |      If not given, all classes are supposed to have weight one.\n",
      " |  \n",
      " |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      " |      weights inversely proportional to class frequencies in the input data\n",
      " |      as ``n_samples / (n_classes * np.bincount(y))``.\n",
      " |  \n",
      " |      Note that these weights will be multiplied with sample_weight (passed\n",
      " |      through the fit method) if sample_weight is specified.\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         *class_weight='balanced'*\n",
      " |  \n",
      " |  random_state : int, RandomState instance, default=None\n",
      " |      Used when ``solver`` == 'sag', 'saga' or 'liblinear' to shuffle the\n",
      " |      data. See :term:`Glossary <random_state>` for details.\n",
      " |  \n",
      " |  solver : {'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'},             default='lbfgs'\n",
      " |  \n",
      " |      Algorithm to use in the optimization problem. Default is 'lbfgs'.\n",
      " |      To choose a solver, you might want to consider the following aspects:\n",
      " |  \n",
      " |          - For small datasets, 'liblinear' is a good choice, whereas 'sag'\n",
      " |            and 'saga' are faster for large ones;\n",
      " |          - For multiclass problems, only 'newton-cg', 'sag', 'saga' and\n",
      " |            'lbfgs' handle multinomial loss;\n",
      " |          - 'liblinear' is limited to one-versus-rest schemes.\n",
      " |  \n",
      " |      .. warning::\n",
      " |         The choice of the algorithm depends on the penalty chosen:\n",
      " |         Supported penalties by solver:\n",
      " |  \n",
      " |         - 'newton-cg'   -   ['l2', 'none']\n",
      " |         - 'lbfgs'       -   ['l2', 'none']\n",
      " |         - 'liblinear'   -   ['l1', 'l2']\n",
      " |         - 'sag'         -   ['l2', 'none']\n",
      " |         - 'saga'        -   ['elasticnet', 'l1', 'l2', 'none']\n",
      " |  \n",
      " |      .. note::\n",
      " |         'sag' and 'saga' fast convergence is only guaranteed on\n",
      " |         features with approximately the same scale. You can\n",
      " |         preprocess the data with a scaler from :mod:`sklearn.preprocessing`.\n",
      " |  \n",
      " |      .. seealso::\n",
      " |         Refer to the User Guide for more information regarding\n",
      " |         :class:`LogisticRegression` and more specifically the\n",
      " |         `Table <https://scikit-learn.org/dev/modules/linear_model.html#logistic-regression>`_\n",
      " |         summarazing solver/penalty supports.\n",
      " |         <!--\n",
      " |         # noqa: E501\n",
      " |         -->\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         Stochastic Average Gradient descent solver.\n",
      " |      .. versionadded:: 0.19\n",
      " |         SAGA solver.\n",
      " |      .. versionchanged:: 0.22\n",
      " |          The default solver changed from 'liblinear' to 'lbfgs' in 0.22.\n",
      " |  \n",
      " |  max_iter : int, default=100\n",
      " |      Maximum number of iterations taken for the solvers to converge.\n",
      " |  \n",
      " |  multi_class : {'auto', 'ovr', 'multinomial'}, default='auto'\n",
      " |      If the option chosen is 'ovr', then a binary problem is fit for each\n",
      " |      label. For 'multinomial' the loss minimised is the multinomial loss fit\n",
      " |      across the entire probability distribution, *even when the data is\n",
      " |      binary*. 'multinomial' is unavailable when solver='liblinear'.\n",
      " |      'auto' selects 'ovr' if the data is binary, or if solver='liblinear',\n",
      " |      and otherwise selects 'multinomial'.\n",
      " |  \n",
      " |      .. versionadded:: 0.18\n",
      " |         Stochastic Average Gradient descent solver for 'multinomial' case.\n",
      " |      .. versionchanged:: 0.22\n",
      " |          Default changed from 'ovr' to 'auto' in 0.22.\n",
      " |  \n",
      " |  verbose : int, default=0\n",
      " |      For the liblinear and lbfgs solvers set verbose to any positive\n",
      " |      number for verbosity.\n",
      " |  \n",
      " |  warm_start : bool, default=False\n",
      " |      When set to True, reuse the solution of the previous call to fit as\n",
      " |      initialization, otherwise, just erase the previous solution.\n",
      " |      Useless for liblinear solver. See :term:`the Glossary <warm_start>`.\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         *warm_start* to support *lbfgs*, *newton-cg*, *sag*, *saga* solvers.\n",
      " |  \n",
      " |  n_jobs : int, default=None\n",
      " |      Number of CPU cores used when parallelizing over classes if\n",
      " |      multi_class='ovr'\". This parameter is ignored when the ``solver`` is\n",
      " |      set to 'liblinear' regardless of whether 'multi_class' is specified or\n",
      " |      not. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n",
      " |      context. ``-1`` means using all processors.\n",
      " |      See :term:`Glossary <n_jobs>` for more details.\n",
      " |  \n",
      " |  l1_ratio : float, default=None\n",
      " |      The Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``. Only\n",
      " |      used if ``penalty='elasticnet'``. Setting ``l1_ratio=0`` is equivalent\n",
      " |      to using ``penalty='l2'``, while setting ``l1_ratio=1`` is equivalent\n",
      " |      to using ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a\n",
      " |      combination of L1 and L2.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  \n",
      " |  classes_ : ndarray of shape (n_classes, )\n",
      " |      A list of class labels known to the classifier.\n",
      " |  \n",
      " |  coef_ : ndarray of shape (1, n_features) or (n_classes, n_features)\n",
      " |      Coefficient of the features in the decision function.\n",
      " |  \n",
      " |      `coef_` is of shape (1, n_features) when the given problem is binary.\n",
      " |      In particular, when `multi_class='multinomial'`, `coef_` corresponds\n",
      " |      to outcome 1 (True) and `-coef_` corresponds to outcome 0 (False).\n",
      " |  \n",
      " |  intercept_ : ndarray of shape (1,) or (n_classes,)\n",
      " |      Intercept (a.k.a. bias) added to the decision function.\n",
      " |  \n",
      " |      If `fit_intercept` is set to False, the intercept is set to zero.\n",
      " |      `intercept_` is of shape (1,) when the given problem is binary.\n",
      " |      In particular, when `multi_class='multinomial'`, `intercept_`\n",
      " |      corresponds to outcome 1 (True) and `-intercept_` corresponds to\n",
      " |      outcome 0 (False).\n",
      " |  \n",
      " |  n_features_in_ : int\n",
      " |      Number of features seen during :term:`fit`.\n",
      " |  \n",
      " |      .. versionadded:: 0.24\n",
      " |  \n",
      " |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      " |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      " |      has feature names that are all strings.\n",
      " |  \n",
      " |      .. versionadded:: 1.0\n",
      " |  \n",
      " |  n_iter_ : ndarray of shape (n_classes,) or (1, )\n",
      " |      Actual number of iterations for all classes. If binary or multinomial,\n",
      " |      it returns only 1 element. For liblinear solver, only the maximum\n",
      " |      number of iteration across all classes is given.\n",
      " |  \n",
      " |      .. versionchanged:: 0.20\n",
      " |  \n",
      " |          In SciPy <= 1.0.0 the number of lbfgs iterations may exceed\n",
      " |          ``max_iter``. ``n_iter_`` will now report at most ``max_iter``.\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  SGDClassifier : Incrementally trained logistic regression (when given\n",
      " |      the parameter ``loss=\"log\"``).\n",
      " |  LogisticRegressionCV : Logistic regression with built-in cross validation.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The underlying C implementation uses a random number generator to\n",
      " |  select features when fitting the model. It is thus not uncommon,\n",
      " |  to have slightly different results for the same input data. If\n",
      " |  that happens, try with a smaller tol parameter.\n",
      " |  \n",
      " |  Predict output may not match that of standalone liblinear in certain\n",
      " |  cases. See :ref:`differences from liblinear <liblinear_differences>`\n",
      " |  in the narrative documentation.\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  \n",
      " |  L-BFGS-B -- Software for Large-scale Bound-constrained Optimization\n",
      " |      Ciyou Zhu, Richard Byrd, Jorge Nocedal and Jose Luis Morales.\n",
      " |      http://users.iems.northwestern.edu/~nocedal/lbfgsb.html\n",
      " |  \n",
      " |  LIBLINEAR -- A Library for Large Linear Classification\n",
      " |      https://www.csie.ntu.edu.tw/~cjlin/liblinear/\n",
      " |  \n",
      " |  SAG -- Mark Schmidt, Nicolas Le Roux, and Francis Bach\n",
      " |      Minimizing Finite Sums with the Stochastic Average Gradient\n",
      " |      https://hal.inria.fr/hal-00860051/document\n",
      " |  \n",
      " |  SAGA -- Defazio, A., Bach F. & Lacoste-Julien S. (2014).\n",
      " |      SAGA: A Fast Incremental Gradient Method With Support\n",
      " |      for Non-Strongly Convex Composite Objectives\n",
      " |      https://arxiv.org/abs/1407.0202\n",
      " |  \n",
      " |  Hsiang-Fu Yu, Fang-Lan Huang, Chih-Jen Lin (2011). Dual coordinate descent\n",
      " |      methods for logistic regression and maximum entropy models.\n",
      " |      Machine Learning 85(1-2):41-75.\n",
      " |      https://www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.datasets import load_iris\n",
      " |  >>> from sklearn.linear_model import LogisticRegression\n",
      " |  >>> X, y = load_iris(return_X_y=True)\n",
      " |  >>> clf = LogisticRegression(random_state=0).fit(X, y)\n",
      " |  >>> clf.predict(X[:2, :])\n",
      " |  array([0, 0])\n",
      " |  >>> clf.predict_proba(X[:2, :])\n",
      " |  array([[9.8...e-01, 1.8...e-02, 1.4...e-08],\n",
      " |         [9.7...e-01, 2.8...e-02, ...e-08]])\n",
      " |  >>> clf.score(X, y)\n",
      " |  0.97...\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      LogisticRegression\n",
      " |      sklearn.linear_model._base.LinearClassifierMixin\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      sklearn.linear_model._base.SparseCoefMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, penalty='l2', *, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='lbfgs', max_iter=100, multi_class='auto', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Fit the model according to the given training data.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          Training vector, where `n_samples` is the number of samples and\n",
      " |          `n_features` is the number of features.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,)\n",
      " |          Target vector relative to X.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,) default=None\n",
      " |          Array of weights that are assigned to individual samples.\n",
      " |          If not provided, then each sample is given unit weight.\n",
      " |      \n",
      " |          .. versionadded:: 0.17\n",
      " |             *sample_weight* support to LogisticRegression.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |          Fitted estimator.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The SAGA solver supports both float64 and float32 bit arrays.\n",
      " |  \n",
      " |  predict_log_proba(self, X)\n",
      " |      Predict logarithm of probability estimates.\n",
      " |      \n",
      " |      The returned estimates for all classes are ordered by the\n",
      " |      label of classes.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Vector to be scored, where `n_samples` is the number of samples and\n",
      " |          `n_features` is the number of features.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      T : array-like of shape (n_samples, n_classes)\n",
      " |          Returns the log-probability of the sample for each class in the\n",
      " |          model, where classes are ordered as they are in ``self.classes_``.\n",
      " |  \n",
      " |  predict_proba(self, X)\n",
      " |      Probability estimates.\n",
      " |      \n",
      " |      The returned estimates for all classes are ordered by the\n",
      " |      label of classes.\n",
      " |      \n",
      " |      For a multi_class problem, if multi_class is set to be \"multinomial\"\n",
      " |      the softmax function is used to find the predicted probability of\n",
      " |      each class.\n",
      " |      Else use a one-vs-rest approach, i.e calculate the probability\n",
      " |      of each class assuming it to be positive using the logistic function.\n",
      " |      and normalize these values across all the classes.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Vector to be scored, where `n_samples` is the number of samples and\n",
      " |          `n_features` is the number of features.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      T : array-like of shape (n_samples, n_classes)\n",
      " |          Returns the probability of the sample for each class in the model,\n",
      " |          where classes are ordered as they are in ``self.classes_``.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.linear_model._base.LinearClassifierMixin:\n",
      " |  \n",
      " |  decision_function(self, X)\n",
      " |      Predict confidence scores for samples.\n",
      " |      \n",
      " |      The confidence score for a sample is proportional to the signed\n",
      " |      distance of that sample to the hyperplane.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The data matrix for which we want to get the confidence scores.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      scores : ndarray of shape (n_samples,) or (n_samples, n_classes)\n",
      " |          Confidence scores per `(n_samples, n_classes)` combination. In the\n",
      " |          binary case, confidence score for `self.classes_[1]` where >0 means\n",
      " |          this class would be predicted.\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict class labels for samples in X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The data matrix for which we want to get the predictions.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y_pred : ndarray of shape (n_samples,)\n",
      " |          Vector containing the class labels for each sample.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True labels for `X`.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.linear_model._base.SparseCoefMixin:\n",
      " |  \n",
      " |  densify(self)\n",
      " |      Convert coefficient matrix to dense array format.\n",
      " |      \n",
      " |      Converts the ``coef_`` member (back) to a numpy.ndarray. This is the\n",
      " |      default format of ``coef_`` and is required for fitting, so calling\n",
      " |      this method is only required on models that have previously been\n",
      " |      sparsified; otherwise, it is a no-op.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |          Fitted estimator.\n",
      " |  \n",
      " |  sparsify(self)\n",
      " |      Convert coefficient matrix to sparse format.\n",
      " |      \n",
      " |      Converts the ``coef_`` member to a scipy.sparse matrix, which for\n",
      " |      L1-regularized models can be much more memory- and storage-efficient\n",
      " |      than the usual numpy.ndarray representation.\n",
      " |      \n",
      " |      The ``intercept_`` member is not converted.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |          Fitted estimator.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n",
      " |      this may actually *increase* memory usage, so use this method with\n",
      " |      care. A rule of thumb is that the number of zero elements, which can\n",
      " |      be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n",
      " |      to provide significant benefits.\n",
      " |      \n",
      " |      After calling this method, further fitting with the partial_fit\n",
      " |      method (if any) will not work until you call densify.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#help(LogisticRegression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba76287c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e5511216",
   "metadata": {},
   "source": [
    "# Prediction for all Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5ec3be23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,\n",
       "       0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
       "       1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Actual output\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b3a948f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe49581e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cab984b7",
   "metadata": {},
   "source": [
    "# Evaluating Model - CONFUSION MATRIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5b2545b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      " [[61  0]\n",
      " [20 19]]\n",
      "Accuracy of the model: 80.0%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix: \\n\", cm)\n",
    "\n",
    "acc_score = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy of the model: {acc_score*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5514bfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0bc158",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c655a54b",
   "metadata": {},
   "source": [
    "# Predicting, whether new customer with Age & Salary will Buy or Not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b1349feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter new customer age: 34\n",
      "enter new customer salary: 50000\n",
      "Customer will buy the product\n"
     ]
    }
   ],
   "source": [
    "age = int(input(\"enter new customer age: \"))\n",
    "salary = int(input(\"enter new customer salary: \"))\n",
    "newCust = [[age, salary]] # nested list here for combining the value\n",
    "result = model.predict(sc.transform(newCust))\n",
    "# print(result)\n",
    "if result== 1:\n",
    "    print(\"Customer will buy the product\")\n",
    "else:\n",
    "    print(\"Customer won't buy the product\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f5da7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adc219d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
